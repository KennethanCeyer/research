{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Review_SungMin Han_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KennethanCeyer/research/blob/master/DL/transformer/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBh04XhcOa-"
      },
      "source": [
        "# Code Review: Transformer\n",
        "\n",
        "## Credits\n",
        "\n",
        "- **Presentor**: JaeYoung Lee\n",
        "- **Reviewer**: SungMin Han\n",
        "- **Source code was copied from:** https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "## Paper information\n",
        "\n",
        "- https://arxiv.org/abs/1706.03762\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4X9Tik3F6VW"
      },
      "source": [
        "# Cell 1\n",
        "\n",
        "This section covers most of the model architectures discussed in Attention Is All You Need, such as the definition of the Transformer encoder and decoder, the implementation of MultiHeadAttention and Positional Encoding.\n",
        "\n",
        "Comments on each implementation are specified for each block, and the utility functions are separated into separate blocks and explained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxIjVahJGAju"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Preload a list of packages for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmZb5T7XF-XL"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.init import constant_\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LayerNorm\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.modules.linear import _LinearWithBias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtnKbmShIAt_"
      },
      "source": [
        "## Utility functions\n",
        "\n",
        "Afterwards, functions frequently used in the code are bundled and provided as utility functions. The feature of each function is as follows.\n",
        "\n",
        "function name | description\n",
        "--------------|--------------\n",
        "`_get_clones()` | It duplicates the layers of each encoder and decoder by the number of given arguments N.\n",
        "`_get_activation_fn()` | Returns the active function defined in torch.nn according to the active function name ('relu', 'gelu') provided as a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxYDl2osGckz"
      },
      "source": [
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return F.gelu\n",
        "\n",
        "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EeGnjPJDUA"
      },
      "source": [
        "# MultiheadAttention\n",
        "\n",
        "Multi-head attention is the process of obtaining and concatinating the separated `Q`, `K`, `V` by separating by `h` to perform the attention operation in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjSuKEb8GJxr"
      },
      "source": [
        "class MultiheadAttention(Module):\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Separate embeds by the number of heads performing multi-head attention.\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Separate dim parameters are defined for Q, K, and V weights.\n",
        "        if self._qkv_same_embed_dim is False:\n",
        "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
        "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
        "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "        # Since Q, K, and V are the same dimension, we manage Tensor of embed_size * 3, embed_size as input projection.\n",
        "            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        # If there is a bias, it is defined as a tensor of size embed_dim.\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "\n",
        "        # Applies a linear transformation to the embed_dim data by using torch.nn.Linear(..., bias=True)\n",
        "        self.out_proj = _LinearWithBias(embed_dim, embed_dim)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Initialize the weights from the sampled criteria U(âˆ’a,a).\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        # Initialize each bias appropriately.\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super(MultiheadAttention, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None,\n",
        "                need_weights=True, attn_mask=None):\n",
        "        # Here, multi-head attention operation is performed through the torch's multi_head_attention_forward method.\n",
        "        # So I'll read https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py separately and review it further.\n",
        "        # First, if Q, K, and V are all the same (like embed_dim), we calculate q, k, and v corresponding to the head through linear operation based on the input projection and bias. (https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L4131)\n",
        "        # If they are not equal to each other, the weight and bias of each tensor area are calculated linearly to obtain Q, K, and V corresponding to the head. (https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L4160-L4185)\n",
        "        # Scale for Q. Scale is done by multiplying pow(dim size, -0.5). (https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L4207)\n",
        "        # We later apply triu to the chunk sliced input value of bptt through `generate_square_subsequent_mask` and provide the mask input src_mask as attn_mask.\n",
        "        # In torch's multi-head attention, the weight is masked for the next token with value -inf through the corresponding attn_mask through masked_fill (https://github.com/pytorch/pytorch/blob/master/torch/nn/functional. py#L4282-L4286)\n",
        "        # This helps the transformer to understand the context by giving a little more attention to the previous word when understanding the sentence.\n",
        "        # The masked weight is used as an argument to the softmax function and a dropout is applied afterwards. (https://github.com/pytorch/pytorch/blob/671ee71ad4b6f507218d1cad278a8e743780b716/torch/nn/functional.py#L4297-L4299)\n",
        "        # The weight is finally multiplied by V to get attention. (https://github.com/pytorch/pytorch/blob/671ee71ad4b6f507218d1cad278a8e743780b716/torch/nn/functional.py#L4301-L4304)\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight)\n",
        "        else:\n",
        "            return F.multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLYYyVELJJPL"
      },
      "source": [
        "# TransformerEncoder\n",
        "\n",
        "A TransformerEncoder consisting of `N` TransformerEncoderLayers is calculated by sequentially using the output returned from the model as the input value of the next layer in the forward process. This process is a sequential operation and cannot be processed in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXOrawjsGNMy"
      },
      "source": [
        "class TransformerEncoder(Module):\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        output = src\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-XvuALZJLlJ"
      },
      "source": [
        "# TransformerDecoder\n",
        "\n",
        "It is similar in shape to `TransformerEncoder`. A TransformerDecoder consisting of `N` TransformerDecoderLayers is calculated using the target inputs sequentially returned from the model in the forward process as the tgt value of the next layer. This process is a sequential operation and cannot be processed in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDY2lW7nGPd3"
      },
      "source": [
        "class TransformerDecoder(Module):\n",
        "    __constants__ = ['norm']\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        output = tgt\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
        "                         memory_mask=memory_mask,\n",
        "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                         memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLUEnY9dJNeP"
      },
      "source": [
        "# TransformerEncoderLayer\n",
        "\n",
        "The encoder has 2 sub-layers. The first layer performs multi-head attention and normalizes the residual connection `src` and attention dropout values. In the second layer, feed forward is performed and the result is applied as a dropout. Once again, it is added with the residual connection `src` and normalized to obtain the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLptY_tlGL4r"
      },
      "source": [
        "class TransformerEncoderLayer(Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "\n",
        "        # Hidden unit = 2048\n",
        "        # Activation = relu\n",
        "        # Output unit = d_model(200)\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        # Norm(Residual connection value(src) + Multi-head attention(src2))\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Norm(Residual connection value(src) + Feed-forward(src))\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eua-N9UKJQj9"
      },
      "source": [
        "# TransformerDecoderLayer\n",
        "\n",
        "The decoder also has two sub layers and additional 1 more layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5J15i2cGUMx"
      },
      "source": [
        "class TransformerDecoderLayer(Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = Linear(d_model, dim_feedforward)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.linear2 = Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.norm3 = LayerNorm(d_model)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.dropout3 = Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if 'activation' not in state:\n",
        "            state['activation'] = F.relu\n",
        "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErON7staJS0M"
      },
      "source": [
        "# PositionalEncoding\n",
        "\n",
        "Unlike RNN, positional encoding(PE) is performed to register the order information of the input data in the Transformer model that processes input values in parallel. In this case, a function that provides frequencies such as `sin()` and `cos()` is used.\n",
        "\n",
        "`max_len` is the number of tokens given and `d_model` is the size of the embedding dimension. Positional Encoding provides information on the order of embedding through a linear operation of a sinusoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yYk8jcYGXgK"
      },
      "source": [
        "class PositionalEncoding(Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Defines embedding space of d_model dimension size for max_len statement\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Defines a vector position representing pos information that is sequential by a given size of max_len\n",
        "        # and adds a dimension for the dot product.\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # In this reference, the natural logarithm of the PE div_term is obtained through exp,\n",
        "        # however, it can also be obtained by powing 2i/d_model for 10,000 according to the Attention Is All You Need paper.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sin to even numbered embeds\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cos to odd numbered embeds\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add PE with input to put the positional information to the input embedding\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1on2zRlqGgYq"
      },
      "source": [
        "# Cell 2\n",
        "\n",
        "## Purpose\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krld8nqtRghU"
      },
      "source": [
        "# Cell 2\n",
        "\n",
        "#####################################################################################################\n",
        "# class variable        model variable        description\n",
        "# (Cell2)               (Cell5)\n",
        "#####################################################################################################\n",
        "# ntoken                ntokens               size of vocabulary\n",
        "# ninp                  emsize                embedding dimension\n",
        "# nhead                 nhead                 the number of heads in the multiheadattentio models\n",
        "# nhid                  nhid                  the dimension of the feedforward network model \n",
        "# nlayers               nlayers               the number of TransformerEncoder\n",
        "# dropout               dropout               the dropout value\n",
        "#####################################################################################################\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAhgXCJjG0Fa"
      },
      "source": [
        "# Cell 3\n",
        "\n",
        "Cell 3 goes through the process of preparing a dataset for training or testing the Transformer. The dataset is based on torchtext and uses the WikiText2 dataset. Specify the batch size of `20`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdsdveEfRpdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530db263-68f7-4ab3-c5ef-3abb1a254a41"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.48M/4.48M [00:00<00:00, 8.61MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QaJIK_KG34j"
      },
      "source": [
        "# Cell 4\n",
        "\n",
        "Generates an input value and a target value as much as bptt of chunk size for evaluation. For the Wiki2 dataset, the sequence is created by slicing the input values `data` and `sequence` as much as`bptt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ3m-reRRrpF"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "\n",
        "    # It is used for cross entropy, so adjust the dimensions for that.\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4gyN1CG6EO"
      },
      "source": [
        "# Cell 5\n",
        "\n",
        "This is the section that defines the main parameters used in the Transformer model.\n",
        "The descriptions of the main parameters are as follows.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "**ntokens**\n",
        "\n",
        "The total number of tokens in the dataset text, defined as the total number of tokens in the vocab in the current Wiki2 dataset, `28871`.\n",
        "\n",
        "**emsize**\n",
        "\n",
        "As word embedding used in Transformer, an embedding vector consisting of `200` dimensions is used. The paper uses a size of 512, but the number of source input tokens is relatively small compared to that of the paper, so the code uses a smaller value.\n",
        "\n",
        "**nlayers**\n",
        "\n",
        "The number of layers of the transformer's encoder decoder. A layer is a structure that has all its shape completely but does not share weight. Each layer has two sub-layers. Instead of the number of 6 layers specified in the paper, the number of layers was set to `2` for each of Encoder and Decoder for testing.\n",
        "\n",
        "**nhead**\n",
        "\n",
        "This is the number of heads used in the multi-head attention mechanism. The larger the number of heads, the better the project progresses and performs parallel operation. Therefore, when there are many core resources, if the number of heads is proportional to the resources of the core, you can get the advantage of computational performance. Here, it is designated as `2` for testing purposes.\n",
        "\n",
        "**dropout**\n",
        "\n",
        "Dropout factor value used in the feed-forward process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1z9gqr1Rti1"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi)  # the size of vocabulary\n",
        "emsize = 200                    # embedding dimension\n",
        "nhid = 200                      # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2                     # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2                       # the number of heads in the multiheadattention models\n",
        "dropout = 0.2                   # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "\n",
        "#####################################################################################################\n",
        "# class variable        model variable        description\n",
        "#####################################################################################################\n",
        "# ntoken                ntokens               size of vocabulary\n",
        "# ninp                  emsize                embedding dimension\n",
        "# nhead                 nhead                 the number of heads in the multiheadattentio models\n",
        "# nhid                  nhid                  the dimension of the feedforward network model \n",
        "# nlayers               nlayers               the number of TransformerEncoder\n",
        "# dropout               dropout               the dropout value\n",
        "#####################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcKVzKhRG8G7"
      },
      "source": [
        "# Cell 6\n",
        "\n",
        "In that section, we train batch data for Wiki2 and evaluate the loss. For training and evaluation of the model, `train()` and `evaluate()` functions are provided respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3U0ZTttRvZt"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if data.size(0) != bptt:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZevJXeuIG-z1"
      },
      "source": [
        "# Cell 7\n",
        "\n",
        "Train the model by iterating `3` epochs, and record the optimally trained model as `best_model`. I did not change the source code because I commented on the reference code, but since the `model` is an object and the field of the object itself is updated in the train, the error that `best_model` looks at only the last object state as a reference is expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpvFAtmFRwDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0aa8d71-edd9-4349-81c4-8ebd43843629"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch 18.02 | loss  7.39 | ppl  1619.44\n",
            "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch 16.79 | loss  6.29 | ppl   541.46\n",
            "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch 16.60 | loss  5.97 | ppl   393.34\n",
            "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch 16.68 | loss  5.81 | ppl   334.41\n",
            "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch 16.84 | loss  5.81 | ppl   333.84\n",
            "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch 17.03 | loss  5.75 | ppl   314.98\n",
            "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch 16.96 | loss  5.71 | ppl   301.03\n",
            "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch 16.87 | loss  5.60 | ppl   271.56\n",
            "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch 17.10 | loss  5.63 | ppl   278.36\n",
            "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch 17.07 | loss  5.63 | ppl   279.01\n",
            "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch 17.17 | loss  5.57 | ppl   262.34\n",
            "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch 17.07 | loss  5.46 | ppl   234.50\n",
            "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch 16.87 | loss  5.53 | ppl   253.15\n",
            "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch 16.96 | loss  5.52 | ppl   250.37\n",
            "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch 16.98 | loss  5.43 | ppl   227.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 57.33s | valid loss  5.12 | valid ppl   167.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 3195 batches | lr 4.51 | ms/batch 17.45 | loss  5.39 | ppl   218.75\n",
            "| epoch   2 |   400/ 3195 batches | lr 4.51 | ms/batch 17.24 | loss  5.38 | ppl   216.00\n",
            "| epoch   2 |   600/ 3195 batches | lr 4.51 | ms/batch 17.13 | loss  5.27 | ppl   194.89\n",
            "| epoch   2 |   800/ 3195 batches | lr 4.51 | ms/batch 17.24 | loss  5.25 | ppl   191.02\n",
            "| epoch   2 |  1000/ 3195 batches | lr 4.51 | ms/batch 17.43 | loss  5.33 | ppl   205.49\n",
            "| epoch   2 |  1200/ 3195 batches | lr 4.51 | ms/batch 17.32 | loss  5.30 | ppl   200.30\n",
            "| epoch   2 |  1400/ 3195 batches | lr 4.51 | ms/batch 17.30 | loss  5.32 | ppl   204.28\n",
            "| epoch   2 |  1600/ 3195 batches | lr 4.51 | ms/batch 17.12 | loss  5.24 | ppl   189.46\n",
            "| epoch   2 |  1800/ 3195 batches | lr 4.51 | ms/batch 17.12 | loss  5.30 | ppl   199.71\n",
            "| epoch   2 |  2000/ 3195 batches | lr 4.51 | ms/batch 17.20 | loss  5.31 | ppl   203.35\n",
            "| epoch   2 |  2200/ 3195 batches | lr 4.51 | ms/batch 17.35 | loss  5.26 | ppl   192.57\n",
            "| epoch   2 |  2400/ 3195 batches | lr 4.51 | ms/batch 17.18 | loss  5.15 | ppl   173.12\n",
            "| epoch   2 |  2600/ 3195 batches | lr 4.51 | ms/batch 17.45 | loss  5.25 | ppl   189.77\n",
            "| epoch   2 |  2800/ 3195 batches | lr 4.51 | ms/batch 17.47 | loss  5.24 | ppl   189.37\n",
            "| epoch   2 |  3000/ 3195 batches | lr 4.51 | ms/batch 17.19 | loss  5.16 | ppl   174.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 58.08s | valid loss  5.07 | valid ppl   158.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 3195 batches | lr 4.29 | ms/batch 17.35 | loss  5.16 | ppl   174.98\n",
            "| epoch   3 |   400/ 3195 batches | lr 4.29 | ms/batch 17.32 | loss  5.17 | ppl   175.74\n",
            "| epoch   3 |   600/ 3195 batches | lr 4.29 | ms/batch 17.79 | loss  5.06 | ppl   157.55\n",
            "| epoch   3 |   800/ 3195 batches | lr 4.29 | ms/batch 17.28 | loss  5.05 | ppl   155.45\n",
            "| epoch   3 |  1000/ 3195 batches | lr 4.29 | ms/batch 17.41 | loss  5.13 | ppl   169.60\n",
            "| epoch   3 |  1200/ 3195 batches | lr 4.29 | ms/batch 17.33 | loss  5.12 | ppl   167.95\n",
            "| epoch   3 |  1400/ 3195 batches | lr 4.29 | ms/batch 17.81 | loss  5.15 | ppl   171.67\n",
            "| epoch   3 |  1600/ 3195 batches | lr 4.29 | ms/batch 17.62 | loss  5.07 | ppl   159.04\n",
            "| epoch   3 |  1800/ 3195 batches | lr 4.29 | ms/batch 17.94 | loss  5.11 | ppl   165.21\n",
            "| epoch   3 |  2000/ 3195 batches | lr 4.29 | ms/batch 17.70 | loss  5.14 | ppl   170.47\n",
            "| epoch   3 |  2200/ 3195 batches | lr 4.29 | ms/batch 17.43 | loss  5.10 | ppl   164.21\n",
            "| epoch   3 |  2400/ 3195 batches | lr 4.29 | ms/batch 17.61 | loss  4.98 | ppl   145.80\n",
            "| epoch   3 |  2600/ 3195 batches | lr 4.29 | ms/batch 17.72 | loss  5.08 | ppl   160.14\n",
            "| epoch   3 |  2800/ 3195 batches | lr 4.29 | ms/batch 17.55 | loss  5.07 | ppl   159.63\n",
            "| epoch   3 |  3000/ 3195 batches | lr 4.29 | ms/batch 17.69 | loss  4.99 | ppl   146.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 59.05s | valid loss  5.00 | valid ppl   148.41\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPLz5JVHBeH"
      },
      "source": [
        "# Cell 8\n",
        "\n",
        "Finally, the eval value of `best_model`, which shows the best result, is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4p5V-gTgRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9ca5b1-9a17-446d-a9e4-979c0f1896d8"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.84 | test ppl   126.72\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voEGzFahERya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}